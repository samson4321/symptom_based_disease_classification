# ğŸ©º Disease Prediction Using Machine Learning

### *Symptom-Based Multi-Class Classification*

---

## ğŸ“Œ Overview

This project explores whether **machine learning models** can predict diseases using only:

* **Patient age**
* **Patient gender**
* **Self-reported symptoms**

The task is formulated as a **multi-class classification problem**, where each patient is assigned **one disease label** from many possible disease categories.

> âš ï¸ **Important:**
> This project is **not** intended to build a real medical diagnostic system.
> Its purpose is to **analyze the limitations of machine learning** when the available data lacks sufficient clinical depth.

---

## ğŸ¯ Project Objectives

* Evaluate symptom-based disease prediction using machine learning
* Analyze model behavior in a **high-class, overlapping-feature setting**
* Understand **why models fail**, not just how they perform
* Demonstrate correct ML methodology and interpretation

---

## ğŸ“‚ Dataset Description

* **Source:** Kaggle Healthcare Dataset
* **Data Type:** Structured tabular data
* **Target Variable:** `Disease` (multi-class, many categories)

### ğŸ”¹ Input Features

* **Age**
  Continuous numerical feature (scaled to [0, 1])

* **Gender**
  Categorical feature (one-hot encoded)

* **Symptoms**
  Text-based feature converted into multiple binary indicators

### ğŸ”¹ Removed Columns

* `Patient_ID` â€” identifier only, no predictive value
* `Symptom_Count` â€” redundant after symptom encoding

---

## ğŸ”„ Data Preprocessing Pipeline

All preprocessing steps were carefully designed to ensure **correctness, consistency, and fairness** in model evaluation.

### 1ï¸âƒ£ Age Scaling

* `Age` scaled to **[0, 1]** using **MinMaxScaler**
* Prevents age from dominating distance calculations in KNN

---

### 2ï¸âƒ£ Gender Encoding

* Converted into:

  * `Gender_Female`
  * `Gender_Male`
  * `Gender_Other`
* Boolean values converted to **0 / 1**
* Gender features were **not scaled**

---

### 3ï¸âƒ£ Symptom Encoding

* The `Symptoms` column (comma-separated text) was:

  * Split into individual symptoms
  * Converted into **binary (0/1) one-hot features**
* Original text column removed

---

### 4ï¸âƒ£ Trainâ€“Test Split

* **75% training / 25% testing**
* **Stratified split** used to preserve disease distribution

---

## ğŸ§  Machine Learning Models

### ğŸ”¹ K-Nearest Neighbors (KNN)

* Distance-based classifier
* Evaluated with multiple values of **k**
* Training vs test accuracy plotted to analyze:

  * Overfitting
  * Underfitting
  * Generalization behavior

---

### ğŸ”¹ Random Forest Classifier

* Ensemble-based model
* Used as a **baseline comparison**
* Helps verify whether low performance is:

  * Model-related âŒ
  * Data-related âœ…

---

## ğŸ“Š Evaluation & Visual Analysis

### 1ï¸âƒ£ Class Distribution Analysis

Disease frequencies were analyzed using:

* Raw class counts
* Percentage distribution

**Visualizations included:**

* ğŸŸ  Pie chart â€” overall proportions
* ğŸ”µ Bar chart â€” clearer comparison with many classes

ğŸ“Œ **Observation:**
Classes are **roughly balanced**, indicating that **class imbalance is not the main issue**.

---

### 2ï¸âƒ£ Training vs Test Accuracy Curve (KNN)

A model complexity curve was generated showing:

* Training accuracy vs number of neighbors (**k**)
* Test accuracy vs number of neighbors (**k**)

#### Observed Behavior

* **k = 1** â†’ training accuracy â‰ˆ **100%** (memorization)
* Training accuracy decreases as **k** increases
* Test accuracy remains **very low for all k values**

#### Interpretation

* Small *k* â†’ **overfitting**
* Large *k* â†’ **underfitting**
* No value of *k* leads to good generalization

---

## ğŸ“ˆ Final Results

### ğŸ”¹ Model Accuracy Summary

| Model         | Accuracy |
| ------------- | -------- |
| KNN (best k)  | ~3â€“4%    |
| Random Forest | ~3%      |

---

### ğŸ”¹ Classification Report (Summary)

* Precision, recall, and F1-scores are **consistently low**
* No disease class is predicted reliably
* Overall performance is close to **random guessing**

---

## ğŸ§ª Interpretation of Results

The low accuracy is **not caused by**:

* Incorrect preprocessing âŒ
* Poor model implementation âŒ
* Inconsistent evaluation âŒ

Instead, it reflects **fundamental limitations of the dataset**.

### Key Reasons for Poor Performance

* ğŸ”´ Large number of disease classes
* ğŸ”´ Strong symptom overlap across diseases
* ğŸ”´ High-dimensional, sparse feature space
* ğŸ”´ Lack of clinical depth:

  * No lab test results
  * No severity indicators
  * No medical history
  * No temporal progression

ğŸ“Œ **Core Conclusion:**

> **Symptoms alone are insufficient to reliably distinguish between many diseases.**

---

## â— Key Insight

This project highlights an important machine learning principle:

> **Low accuracy does not necessarily indicate a bad model â€” it may indicate insufficient information.**

Understanding *why* a model fails is a critical part of responsible ML practice.

---

## ğŸš€ Future Work

### ğŸ”§ Data-Level Improvements

* Group diseases into broader medical categories
* Remove very rare or non-informative symptoms
* Incorporate richer clinical features

### ğŸ¤– Model-Level Improvements

* Hierarchical classification
* Gradient boosting models
* Feature selection or dimensionality reduction

### ğŸ“ˆ Evaluation Improvements

* **Top-k accuracy** (e.g., top-5 predictions)
* Confusion matrix analysis for systematic misclassifications

---

## âš ï¸ Disclaimer

This project is intended for **educational and experimental purposes only**.
It must **not** be used for real medical diagnosis or clinical decision-making.

---

## âœ… Final Takeaway

* âœ”ï¸ Code implementation is correct
* âœ”ï¸ Methodology is sound
* âœ”ï¸ Analysis is honest and well-reasoned
* âœ”ï¸ Results reflect real-world ML limitations

This makes the project **scientifically valid and informative**, even with low accuracy.
